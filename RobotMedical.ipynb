{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictorHo1114/Medical-Chatbot/blob/main/RobotMedical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvxVw04Vdzcl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# ÂÆâË£ùÊúÄÊñ∞ Unsloth ‰æùË≥¥\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install opencc-python-reimplemented # Áî®ÊñºÁ∞°ËΩâÁπÅ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from opencc import OpenCC\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 0. Á¢∫‰øù Google Drive Â∑≤ÊéõËºâ (ÊúÄÂÑ™ÂÖàÂü∑Ë°å)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 1. Ë®≠ÂÆöÊ®°ÂûãÂèÉÊï∏ (Áµ±‰∏ÄÂú®ÈÄôË£°Ë®≠ÂÆö)\n",
        "max_seq_length = 1024 # ‚úÇÔ∏è ÈóúÈçµÔºöÁõ¥Êé•ÈéñÊ≠ª 1024ÔºåÁ¢∫‰øùË∑ëÂæóÂãï\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# 2. ËºâÂÖ• Qwen 2.5 3B (Âè™ËºâÂÖ•ÈÄô‰∏ÄÊ¨°ÔºÅ)\n",
        "print(\"üîÑ Ê≠£Âú®ËºâÂÖ•Ê®°Âûã...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. Âä†‰∏ä LoRA ÈÅ©ÈÖçÂô®\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# 4. Ê∫ñÂÇôÊï∏ÊìöËàáÁπÅÈ´îËΩâÊèõ\n",
        "print(\"üìö Ê≠£Âú®ËôïÁêÜÊï∏ÊìöÈõÜ (Âê´ÁπÅÈ´îËΩâÊèõ)...\")\n",
        "cc = OpenCC('s2twp') # Á∞°ËΩâÁπÅ (Âè∞ÁÅ£Ê≠£È´î)\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "‰Ω†ÊòØ‰∏Ä‰ΩçÂ∞àÊ•≠ÁöÑÈÜ´ÁôÇAIÂä©Êâã„ÄÇË´ãË©≥Á¥∞ÈÄ≤Ë°åÊé®ÁêÜ‰∏¶ÂõûÁ≠î‰ΩøÁî®ËÄÖÁöÑÈÜ´ÁôÇÂïèÈ°å„ÄÇ\n",
        "**Ë´ãÂãôÂøÖ‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÔºàTraditional ChineseÔºâÂõûÁ≠î„ÄÇ**\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"Question\"]\n",
        "    cots = examples[\"Complex_CoT\"]\n",
        "    outputs = examples[\"Response\"]\n",
        "    texts = []\n",
        "    for instruction, cot, output in zip(instructions, cots, outputs):\n",
        "        # ÁµÑÂêàÊé®ÁêÜÈèà (CoT) ËàáÊúÄÁµÇÁµêË´ñ\n",
        "        full_response = f\"{cot}\\n\\nÁµêË´ñÔºö{output}\"\n",
        "\n",
        "        # --- ‚ö° Âç≥ÊôÇÁπÅÈ´îËΩâÊèõ ---\n",
        "        instruction_tc = cc.convert(instruction)\n",
        "        full_response_tc = cc.convert(full_response)\n",
        "\n",
        "        text = alpaca_prompt.format(instruction_tc, full_response_tc) + tokenizer.eos_token\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# ËºâÂÖ•Êï∏ÊìöÈõÜ (Ë®òÂæóÈÅ∏ \"zh\")\n",
        "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"zh\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "# 5. Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏ (Ê•µÈÄüÁâà)\n",
        "print(\"‚öôÔ∏è Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏...\")\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length, # ÈÄôË£°ÊúÉËá™Âãï‰ΩøÁî®‰∏äÈù¢Ë®≠ÂÆöÁöÑ 1024\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4, # Èï∑Â∫¶Ê∏õÂçäÂæåÔºåBatch 4 ÊáâË©≤Ë∑ëÂæóÂãï\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps = 200,         # Âè™Ë∑ë 200 Ê≠•\n",
        "        learning_rate = 3e-4,    # ËºÉÈ´òÁöÑÂ≠∏ÁøíÁéá\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "\n",
        "        # --- üíæ Â≠òÊ™î‰øùÈö™ ---\n",
        "        output_dir = \"/content/drive/MyDrive/Medical_Qwen_Checkpoints\",\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 50,\n",
        "        save_total_limit = 2,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 6. ÈñãÂßãË®ìÁ∑¥\n",
        "print(\"üöÄ ÈñãÂßãÊ•µÈÄüË®ìÁ∑¥ÔºÅ\")\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "wqDEaIxI3IFC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ë®≠ÂÆö‰Ω†ÂâõÂâõ‰ª£Á¢º‰∏≠ÁöÑÂ≠òÊ™îË∑ØÂæë (Â¶ÇÊûú‰∏çÁ¢∫ÂÆöÔºåÊàëÂÄëÂÖàÊêúÊï¥ÂÄã Checkpoints Ë≥áÊñôÂ§æ)\n",
        "# Ê†πÊìö‰Ω†‰∏ä‰∏ÄÊÆµ‰ª£Á¢ºÔºåË∑ØÂæëÊáâË©≤ÊòØÈÄôÂÄãÔºö\n",
        "checkpoints_path = \"/content/drive/MyDrive/Medical_Qwen_Checkpoints\"\n",
        "\n",
        "print(f\"üìÇ Ê≠£Âú®Ê™¢Êü•Ë∑ØÂæëÔºö{checkpoints_path}\")\n",
        "\n",
        "if os.path.exists(checkpoints_path):\n",
        "    files = os.listdir(checkpoints_path)\n",
        "    print(f\"‚úÖ ÊâæÂà∞‰∫ÜÔºÅË≥áÊñôÂ§æÂÖßÊúâ {len(files)} ÂÄãÁâ©‰ª∂„ÄÇ\")\n",
        "    print(\"ÂÖßÂÆπÂåÖÂê´Ôºö\", files)\n",
        "\n",
        "    # Ê™¢Êü•ÊòØÂê¶ÊúâÊúÄÊñ∞ÁöÑ checkpoint (‰æãÂ¶Ç checkpoint-200)\n",
        "    checkpoints = [f for f in files if f.startswith(\"checkpoint-\")]\n",
        "    if checkpoints:\n",
        "        print(f\"üèÜ ÊàêÂäüÂ≠òÊ™îÁöÑÊ™¢Êü•ÈªûÔºö{sorted(checkpoints)}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è ÊúâË≥áÊñôÂ§æÔºå‰ΩÜÊ≤íÁúãÂà∞ checkpoint-XXXÔºåË´ãÊ™¢Êü•ÂÖßÂÆπ„ÄÇ\")\n",
        "else:\n",
        "    print(\"‚ùå Ê≤íÊâæÂà∞Ë∑ØÂæëÔºÅÂèØËÉΩÊòØË∑ØÂæëÊâìÈåØÔºåÊàñÈÇÑÊ≤íÂêåÊ≠•„ÄÇ\")"
      ],
      "metadata": {
        "id": "exYTNd6vKOK2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace your_notebook.ipynb"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aoFYlRimIHFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ÂàáÊèõÂà∞Êé®ÁêÜÊ®°Âºè\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 2. Ê∫ñÂÇôÂïèÈ°å (Ê∏¨Ë©¶ÂÆÉÁöÑÈÜ´ÁôÇÁü•Ë≠òËàáÁπÅÈ´îËÉΩÂäõ)\n",
        "question = \"ÈÜ´ÁîüÔºåÊàëÊúÄËøëÊó©‰∏äËµ∑Â∫äÈÉΩÊúÉÈ†≠ÊöàÔºåËÄå‰∏îÊâãÊåáÂ∞ñÊúâÈªûÈ∫ªÈ∫ªÁöÑÔºåÈÄôÊòØ‰∏≠È¢®ÁöÑÂâçÂÖÜÂóéÔºüÊàëÂæàÊìîÂøÉ„ÄÇ\"\n",
        "\n",
        "# 3. Ê†ºÂºèÂåñËº∏ÂÖ•\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "‰Ω†ÊòØ‰∏Ä‰ΩçÂ∞àÊ•≠ÁöÑÈÜ´ÁôÇAIÂä©Êâã„ÄÇË´ãË©≥Á¥∞ÈÄ≤Ë°åÊé®ÁêÜ‰∏¶ÂõûÁ≠î‰ΩøÁî®ËÄÖÁöÑÈÜ´ÁôÇÂïèÈ°å„ÄÇ\n",
        "**Ë´ãÂãôÂøÖ‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáÔºàTraditional ChineseÔºâÂõûÁ≠î„ÄÇ**\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# 4. ÁîüÊàêÂõûÁ≠î\n",
        "inputs = tokenizer(\n",
        "    [alpaca_prompt.format(question)],\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "print(\"ü§ñ ‰Ω†ÁöÑÂ∞àÂ±¨ÈÜ´ÁôÇ AI Ê≠£Âú®ÊÄùËÄÉ‰∏≠...\\n\" + \"=\"*30)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 512, # ËÆìÂÆÉÂ§öË¨õ‰∏ÄÈªû\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        ")\n",
        "\n",
        "# 5. Ëß£Á¢º‰∏¶È°ØÁ§∫\n",
        "result = tokenizer.batch_decode(outputs)[0]\n",
        "# Âè™È°ØÁ§∫ÂõûÁ≠îÁöÑÈÉ®ÂàÜ\n",
        "print(result.split(\"### Response:\\n\")[-1].replace(\"<|endoftext|>\", \"\"))"
      ],
      "metadata": {
        "id": "Je0Me3fmKk8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}